# Project: adams
# Date: 2024-08-23 13:31:53
# User: fracpete
# Charset: UTF-8
# Modules: adams-bootstrapp,adams-compress,adams-core,adams-db,adams-event,adams-excel,adams-heatmap,adams-imaging,adams-imaging-boofcv,adams-jep,adams-json,adams-math,adams-matlab,adams-meta,adams-ml,adams-net,adams-odf,adams-pdf,adams-pyro4,adams-python,adams-r,adams-rats-core,adams-rats-net,adams-rats-redis,adams-rats-rest,adams-rats-webservice,adams-redis,adams-rest,adams-security,adams-spectral-2dim-core,adams-spectral-2dim-handheld,adams-spectral-2dim-r,adams-spectral-2dim-rats,adams-spectral-2dim-webservice,adams-spectral-3way-core,adams-spectral-app,adams-spreadsheet,adams-terminal,adams-visualstats,adams-webservice,adams-webservice-core,adams-weka-lts,adams-xml,adams-yaml
#
adams.flow.control.Flow -annotation "Imports the OSSL L0 spectral data into an ADAMS spectral database.\\nAutomatically downloads and expands the OSSL data if not present.\\nMore information about the OSSL data:\\nhttps://soilspectroscopy.github.io/ossl-manual/" -flow-execution-listener adams.flow.execution.NullListener -flow-restart-manager adams.flow.control.flowrestart.NullManager
 adams.flow.standalone.CallableActors
  adams.flow.sink.Display -name "Imported records" -short-title true -display-type adams.flow.core.displaytype.Default -writer adams.data.io.output.NullWriter
  adams.flow.sink.Display -name "Total records" -short-title true -display-type adams.flow.core.displaytype.Default -x -3 -writer adams.data.io.output.NullWriter
 adams.flow.standalone.SetVariable -name waveno_check_col -annotation "the waveno that is checked to determine whether spectra present or not" -var-name waveno_check_col -var-value 100
 adams.flow.source.Start
 adams.flow.control.Trigger -name urls
  adams.flow.source.StringConstants -output-array true -string https://storage.googleapis.com/soilspec4gg-public/ossl_soilsite_L0_v1.2.csv.gz -string https://storage.googleapis.com/soilspec4gg-public/ossl_soillab_L0_v1.2.csv.gz -string https://storage.googleapis.com/soilspec4gg-public/ossl_mir_L0_v1.2.csv.gz -string https://storage.googleapis.com/soilspec4gg-public/ossl_visnir_L0_v1.2.csv.gz -conversion adams.data.conversion.StringToString
  adams.flow.transformer.SetStorageValue -storage-name urls
 adams.flow.control.Trigger -name "prompt user"
  adams.flow.standalone.SetVariable -name "restore file" -var-name restore -var-value @{flow_filename_long}.props -value-type FILE_FORWARD_SLASHES -expand-value true
  adams.flow.source.EnterManyValues -name Parameters -stop-if-canceled true -value "adams.flow.source.valuedefinition.DirectoryValueDefinition -name output_dir -display \"Output dir\" -help \"whether to download/compress the files to\" -file-chooser-title \"Output dir\" -use-forward-slashes true -use-absolute-path true" -value "adams.flow.source.valuedefinition.DefaultValueDefinition -name buffer_size -display \"Buffer size\" -help \"the size of the buffer for downloading/decompressing\" -type INTEGER -default-value 102400" -value "adams.flow.source.valuedefinition.DefaultValueDefinition -name chunk_size -display \"Chunk size\" -help \"the number of rows to read in one go from the spreadsheet\" -type INTEGER -default-value 100" -value "adams.flow.source.valuedefinition.DefaultValueDefinition -name jdbc_url -display \"JDBC URL\" -help \"JDBC URL of database to import the spectra and sample data to; the database must exist.\" -default-value \"jdbc:mysql://localhost:3306/ossl?useSSL=false&&sessionVariables=sql_mode=\\\'ALLOW_INVALID_DATES\\\'\"" -value "adams.flow.source.valuedefinition.DefaultValueDefinition -name jdbc_user -display \"JDBC user\" -help \"the user to connect with to the database\" -default-value dbuser" -value "adams.flow.source.valuedefinition.DefaultValueDefinition -name jdbc_pw -display \"JDBC password\" -help \"the password to use for connecting to the database\" -type PASSWORD -default-value dbpw" -value "adams.flow.source.valuedefinition.ListSelectionValueDefinition -name import_type -display Importing -help \"what data to import\" -value all -value mir -value visnir -value soillab -value soilsite -default-value mir" -output-type MAP -restoration-enabled true -restoration-file @{restore}
  adams.flow.transformer.MapToVariables
 adams.flow.control.Trigger -name "download/decompress (if necessary)"
  adams.flow.source.StorageValue -storage-name urls -conversion adams.data.conversion.UnknownToUnknown
  adams.flow.transformer.ArrayToSequence
  adams.flow.transformer.SetVariable -var-name url
  adams.flow.control.Tee -name "output file"
   adams.flow.transformer.StringReplace -find .*\\/
   adams.flow.transformer.PrependDir -prefix @{output_dir} -use-forward-slashes true
   adams.flow.transformer.SetVariable -var-name output_file_gz
   adams.flow.transformer.StringReplace -name "StringReplace (2)" -find \\.gz$
   adams.flow.transformer.SetVariable -name "SetVariable (2)" -var-name output_file
  adams.flow.control.ConditionalTrigger -name "file missing?" -condition "adams.flow.condition.bool.Not -condition \"adams.flow.condition.bool.FileExists -file @{output_file_gz} -generator adams.core.io.NullFilenameGenerator\""
   adams.flow.source.URLSupplier -url @{url}
   adams.flow.control.Tee -name info
    adams.flow.transformer.Convert -conversion adams.data.conversion.AnyToString
    adams.flow.transformer.StringInsert -position first -value "downloading: "
    adams.flow.sink.CallableSink -callable "Imported records"
   adams.flow.sink.DownloadFile -output @{output_file_gz} -buffer-size @{buffer_size}
  adams.flow.control.ConditionalTrigger -name "not decompressed?" -condition "adams.flow.condition.bool.Not -condition \"adams.flow.condition.bool.FileExists -file @{output_file} -generator adams.core.io.NullFilenameGenerator\""
   adams.flow.source.FileSupplier -file @{output_file_gz} -use-forward-slashes true
   adams.flow.control.Tee -name info
    adams.flow.transformer.StringInsert -position first -value "decompressing: "
    adams.flow.sink.CallableSink -callable "Imported records"
   adams.flow.transformer.GUNZIP -buffer @{buffer_size}
 adams.flow.control.Trigger -name import
  adams.flow.standalone.DatabaseConnection -url @{jdbc_url} -user @{jdbc_user} -password @{jdbc_pw} -data-type-setup adams.db.datatype.DummySetup
  adams.flow.standalone.Tool -tool adams.tools.InitializeTables
  adams.flow.source.StorageValue -storage-name urls -conversion adams.data.conversion.UnknownToUnknown
  adams.flow.transformer.ArrayToSequence
  adams.flow.transformer.StringReplace -find .*\\/
  adams.flow.transformer.StringReplace -name "StringReplace (2)" -find .gz$
  adams.flow.transformer.PrependDir -prefix @{output_dir} -use-forward-slashes true
  adams.flow.transformer.SetVariable -var-name input_file
  adams.flow.control.Tee -name info
   adams.flow.transformer.StringInsert -position first -value "importing: "
   adams.flow.sink.CallableSink -callable "Imported records"
  adams.flow.control.Tee -name "info (2)"
   adams.flow.transformer.StringInsert -position first -value "importing: "
   adams.flow.sink.CallableSink -callable "Total records"
  adams.flow.control.ConditionalTee -name mir -condition "adams.flow.condition.bool.Expression -expression \"(\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"all\\\\\\\") or (matches(\\\\\\\"X\\\\\\\", \\\\\\\".*mir.*\\\\\\\") and (\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"mir\\\\\\\"))\""
   adams.flow.transformer.SetVariable -name reset -var-name count -var-value 0
   adams.flow.transformer.SetVariable -name "reset (2)" -var-name total -var-value 0
   adams.flow.transformer.SpreadSheetFileReader -reader "adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$ -chunk-size @{chunk_size}"
   adams.flow.control.Tee -name info
    adams.flow.transformer.SpreadSheetInfo
    adams.flow.transformer.SetVariable -var-name rows
    adams.flow.transformer.IncVariable -var-name total -inc-int @{rows} -output-variable-value true
    adams.flow.transformer.Convert -conversion adams.data.conversion.AnyToString
    adams.flow.sink.CallableSink -callable "Total records"
   adams.flow.transformer.SpreadSheetRowFilter -finder "adams.data.spreadsheet.rowfinder.Invert -row-finder \"adams.data.spreadsheet.rowfinder.MissingValue -att-range @{waveno_check_col}\"" -create-view true
   adams.flow.control.Block -condition "adams.flow.condition.bool.Not -condition adams.flow.condition.bool.HasRows"
   adams.flow.control.Tee -name "save chunk"
    adams.flow.sink.SpreadSheetFileWriter -output ${TMP}/ossl.csv -writer adams.data.io.output.CsvSpreadSheetWriter
   adams.flow.control.Trigger -name "process chunk"
    adams.flow.source.FileSupplier -file ${TMP}/ossl.csv
    adams.flow.transformer.SpectrumFileReader -reader "adams.data.io.input.RowWiseSpreadSheetSpectrumReader -format MIR -reader \"adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$ -chunk-size @{chunk_size}\" -row-finder adams.data.spreadsheet.rowfinder.AllFinder -sample-id-column id.layer_uuid_txt -wave-columns 15-last -header-contains-wave-number true -wave-number-regexp scan_mir.([0-9]+)_abs -sample-data-columns 1-14" -output-array true -custom-comparator adams.data.spectrum.SpectrumComparator
    adams.flow.control.Tee -name store
     adams.flow.control.Tee -name "# spectra"
      adams.flow.transformer.ArrayLength
      adams.flow.transformer.SetVariable -var-name records
     adams.flow.transformer.BulkSpectrumDbWriter -batch-size @{chunk_size} -auto-commit false -new-connection true
     adams.flow.transformer.IncVariable -var-name count -inc-int @{records}
    adams.flow.control.Trigger -name info
     adams.flow.source.Variable -var-name count -conversion adams.data.conversion.StringToString
     adams.flow.sink.CallableSink -callable "Imported records"
  adams.flow.control.ConditionalTee -name visnir -condition "adams.flow.condition.bool.Expression -expression \"(\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"all\\\\\\\") or (matches(\\\\\\\"X\\\\\\\", \\\\\\\".*visnir.*\\\\\\\") and (\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"visnir\\\\\\\"))\""
   adams.flow.transformer.SetVariable -name reset -var-name count -var-value 0
   adams.flow.transformer.SetVariable -name "reset (2)" -var-name total -var-value 0
   adams.flow.transformer.SpreadSheetFileReader -reader "adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$ -chunk-size @{chunk_size}"
   adams.flow.control.Tee -name info
    adams.flow.transformer.SpreadSheetInfo
    adams.flow.transformer.SetVariable -var-name rows
    adams.flow.transformer.IncVariable -var-name total -inc-int @{rows} -output-variable-value true
    adams.flow.transformer.Convert -conversion adams.data.conversion.AnyToString
    adams.flow.sink.CallableSink -callable "Total records"
   adams.flow.transformer.SpreadSheetRowFilter -finder "adams.data.spreadsheet.rowfinder.Invert -row-finder \"adams.data.spreadsheet.rowfinder.MissingValue -att-range @{waveno_check_col}\"" -create-view true
   adams.flow.control.Block -condition "adams.flow.condition.bool.Not -condition adams.flow.condition.bool.HasRows"
   adams.flow.control.Tee -name "save chunk"
    adams.flow.sink.SpreadSheetFileWriter -output ${TMP}/ossl.csv -writer adams.data.io.output.CsvSpreadSheetWriter
   adams.flow.control.Trigger -name "process chunk"
    adams.flow.source.FileSupplier -file ${TMP}/ossl.csv
    adams.flow.transformer.SpectrumFileReader -reader "adams.data.io.input.RowWiseSpreadSheetSpectrumReader -format VISNIR -reader \"adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$\" -row-finder adams.data.spreadsheet.rowfinder.AllFinder -sample-id-column id.layer_uuid_txt -wave-columns 15-last -header-contains-wave-number true -wave-number-regexp scan_visnir.([0-9]+)_ref -sample-data-columns 1-14" -output-array true -custom-comparator adams.data.spectrum.SpectrumComparator
    adams.flow.control.Tee -name store
     adams.flow.control.Tee -name "# spectra"
      adams.flow.transformer.ArrayLength
      adams.flow.transformer.SetVariable -var-name records
     adams.flow.transformer.BulkSpectrumDbWriter -batch-size @{chunk_size} -auto-commit false -new-connection true
     adams.flow.transformer.IncVariable -var-name count -inc-int @{records}
    adams.flow.control.Trigger -name info
     adams.flow.source.Variable -var-name count -conversion adams.data.conversion.StringToString
     adams.flow.sink.CallableSink -callable "Imported records"
  adams.flow.control.ConditionalTee -name soillab -condition "adams.flow.condition.bool.Expression -expression \"(\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"all\\\\\\\") or (matches(\\\\\\\"X\\\\\\\", \\\\\\\".*soillab.*\\\\\\\") and (\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"soillab\\\\\\\"))\""
   adams.flow.transformer.SetVariable -name reset -var-name count -var-value 0
   adams.flow.transformer.SetVariable -name "reset (2)" -var-name total -var-value 0
   adams.flow.transformer.SpreadSheetFileReader -reader "adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$ -chunk-size @{chunk_size}"
   adams.flow.control.Tee -name info
    adams.flow.transformer.SpreadSheetInfo
    adams.flow.transformer.SetVariable -var-name rows
    adams.flow.transformer.IncVariable -var-name total -inc-int @{rows} -output-variable-value true
    adams.flow.transformer.Convert -conversion adams.data.conversion.AnyToString
    adams.flow.sink.CallableSink -callable "Total records"
   adams.flow.control.Tee -name "save chunk"
    adams.flow.sink.SpreadSheetFileWriter -output ${TMP}/ossl.csv -writer adams.data.io.output.CsvSpreadSheetWriter
   adams.flow.control.Trigger -name "process chunk"
    adams.flow.source.FileSupplier -file ${TMP}/ossl.csv
    adams.flow.transformer.SampleDataFileReader -output-array true -reader "adams.data.io.input.MultiColumnSpreadSheetSampleDataReader -reader \"adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$\" -col-id id.layer_uuid_txt -cols-sampledata 1,3-last -cols-numeric 3-last -row-finder adams.data.spreadsheet.rowfinder.AllFinder"
    adams.flow.control.Tee -name store
     adams.flow.control.Tee -name "# sample data"
      adams.flow.transformer.ArrayLength
      adams.flow.transformer.SetVariable -var-name records
     adams.flow.transformer.BulkSampleDataDbWriter -data-type STRING -data-type NUMERIC -data-type BOOLEAN -skip-fields true -batch-size @{chunk_size} -auto-commit false -new-connection true
     adams.flow.transformer.IncVariable -var-name count -inc-int @{records}
    adams.flow.control.Trigger -name info
     adams.flow.source.Variable -var-name count -conversion adams.data.conversion.StringToString
     adams.flow.sink.CallableSink -callable "Imported records"
  adams.flow.control.ConditionalTee -name soilsite -condition "adams.flow.condition.bool.Expression -expression \"(\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"all\\\\\\\") or (matches(\\\\\\\"X\\\\\\\", \\\\\\\".*soilsite.*\\\\\\\") and (\\\\\\\"@{import_type}\\\\\\\" = \\\\\\\"soilsite\\\\\\\"))\""
   adams.flow.transformer.SetVariable -name reset -var-name count -var-value 0
   adams.flow.transformer.SetVariable -name "reset (2)" -var-name total -var-value 0
   adams.flow.transformer.SpreadSheetFileReader -reader "adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$ -chunk-size @{chunk_size}"
   adams.flow.control.Tee -name info
    adams.flow.transformer.SpreadSheetInfo
    adams.flow.transformer.SetVariable -var-name rows
    adams.flow.transformer.IncVariable -var-name total -inc-int @{rows} -output-variable-value true
    adams.flow.transformer.Convert -conversion adams.data.conversion.AnyToString
    adams.flow.sink.CallableSink -callable "Total records"
   adams.flow.control.Tee -name "save chunk"
    adams.flow.sink.SpreadSheetFileWriter -output ${TMP}/ossl.csv -writer adams.data.io.output.CsvSpreadSheetWriter
   adams.flow.control.Trigger -name "process chunk"
    adams.flow.source.FileSupplier -file ${TMP}/ossl.csv
    adams.flow.transformer.SampleDataFileReader -output-array true -reader "adams.data.io.input.MultiColumnSpreadSheetSampleDataReader -reader \"adams.data.io.input.FastCsvSpreadSheetReader -data-row-type adams.data.spreadsheet.DenseDataRow -spreadsheet-type adams.data.spreadsheet.DefaultSpreadSheet -missing ^$\" -col-id id.layer_uuid_txt -cols-sampledata 1,3-last -cols-numeric 4-8,17-19 -row-finder adams.data.spreadsheet.rowfinder.AllFinder"
    adams.flow.control.Tee -name store
     adams.flow.control.Tee -name "# sample data"
      adams.flow.transformer.ArrayLength
      adams.flow.transformer.SetVariable -var-name records
     adams.flow.transformer.BulkSampleDataDbWriter -data-type STRING -data-type NUMERIC -data-type BOOLEAN -skip-fields true -batch-size @{chunk_size} -auto-commit false -new-connection true
     adams.flow.transformer.IncVariable -var-name count -inc-int @{records}
    adams.flow.control.Trigger -name info
     adams.flow.source.Variable -var-name count -conversion adams.data.conversion.StringToString
     adams.flow.sink.CallableSink -callable "Imported records"